{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.10), please consider upgrading to the latest version (0.3.11).\n",
      "Path to dataset files: /Users/anatol/.cache/kagglehub/datasets/ellipticco/elliptic-data-set/versions/1\n"
     ]
    }
   ],
   "source": [
    "## Downloading the database\n",
    "import kagglehub\n",
    "\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"ellipticco/elliptic-data-set\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        txId    class\n",
      "0  230425980  unknown\n",
      "1    5530458  unknown\n",
      "2  232022460  unknown\n",
      "3  232438397        2\n",
      "4  230460314  unknown\n",
      "\n",
      "   230425980  1  -0.1714692896288031  -0.18466755143291433  \\\n",
      "0    5530458  1            -0.171484             -0.184668   \n",
      "1  232022460  1            -0.172107             -0.184668   \n",
      "2  232438397  1             0.163054              1.963790   \n",
      "3  230460314  1             1.011523             -0.081127   \n",
      "4  230459870  1             0.961040             -0.081127   \n",
      "\n",
      "   -1.2013688016765636  -0.12196959975910057  -0.04387454791734898  \\\n",
      "0            -1.201369             -0.121970             -0.043875   \n",
      "1            -1.201369             -0.121970             -0.043875   \n",
      "2            -0.646376             12.409294             -0.063725   \n",
      "3            -1.201369              1.153668              0.333276   \n",
      "4            -1.201369              1.303743              0.333276   \n",
      "\n",
      "   -0.11300200928476244  -0.06158379407303222  -0.16209679981659642  ...  \\\n",
      "0             -0.113002             -0.061584             -0.162112  ...   \n",
      "1             -0.113002             -0.061584             -0.162749  ...   \n",
      "2              9.782742             12.414558             -0.163645  ...   \n",
      "3              1.312656             -0.061584             -0.163523  ...   \n",
      "4              1.480381             -0.061584             -0.163577  ...   \n",
      "\n",
      "   -0.5621534802884299  -0.6009988905192808  1.4613303209554889  \\\n",
      "0             0.947382             0.673103           -0.979074   \n",
      "1             0.670883             0.439728           -0.979074   \n",
      "2            -0.577099            -0.613614            0.241128   \n",
      "3            -0.511871            -0.400422            0.517257   \n",
      "4            -0.504702            -0.422589           -0.226790   \n",
      "\n",
      "   1.4613689382001922  0.01827940003744589  -0.0874901561101501  \\\n",
      "0           -0.978556             0.018279            -0.087490   \n",
      "1           -0.978556            -0.098889            -0.106715   \n",
      "2            0.241406             1.072793             0.085530   \n",
      "3            0.579382             0.018279             0.277775   \n",
      "4           -0.117629             0.018279             0.277775   \n",
      "\n",
      "   -0.13115530389558736  -0.09752359377152515  -0.12061340670311574  \\\n",
      "0             -0.131155             -0.097524             -0.120613   \n",
      "1             -0.131155             -0.183671             -0.120613   \n",
      "2             -0.131155              0.677799             -0.120613   \n",
      "3              0.326394              1.293750              0.178136   \n",
      "4              0.413931              1.149556             -0.696053   \n",
      "\n",
      "   -0.11979245961251665  \n",
      "0             -0.119792  \n",
      "1             -0.119792  \n",
      "2             -0.119792  \n",
      "3              0.179117  \n",
      "4             -0.695540  \n",
      "\n",
      "[5 rows x 167 columns]\n",
      "\n",
      "       txId1      txId2\n",
      "0  230425980    5530458\n",
      "1  232022460  232438397\n",
      "2  230460314  230459870\n",
      "3  230333930  230595899\n",
      "4  232013274  232029206\n"
     ]
    }
   ],
   "source": [
    "classes_df = pd.read_csv(path + \"/elliptic_bitcoin_dataset/elliptic_txs_classes.csv\")\n",
    "features_df = pd.read_csv(path + \"/elliptic_bitcoin_dataset/elliptic_txs_features.csv\")\n",
    "edge_df = pd.read_csv(path + \"/elliptic_bitcoin_dataset/elliptic_txs_edgelist.csv\")\n",
    "\n",
    "print(classes_df.head(), end=\"\\n\\n\")\n",
    "print(features_df.head(), end=\"\\n\\n\")\n",
    "print(edge_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['txId', 'Time step', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8',\n",
      "       ...\n",
      "       'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164',\n",
      "       'f165'],\n",
      "      dtype='object', length=167)\n"
     ]
    }
   ],
   "source": [
    "feature_headers = ['txId', 'Time step'] + [f'f{i}' for i in range(1, 166)]\n",
    "features_df.columns = feature_headers\n",
    "print(features_df.columns)\n",
    "df = pd.merge(features_df, classes_df, on='txId', how='left')\n",
    "\n",
    "# Map classes for evaluation: 1 (illicit) -> 1, 2 (licit) -> 0, unknown -> NaN\n",
    "df['class'] = df['class'].map({'1': 1, '2': 0})\n",
    "known_indices = df.index[df['class'].notna()].tolist()\n",
    "y_known = df.loc[known_indices, 'class'].values.astype(int)\n",
    "# Map classes for evaluation: 1 (illicit) -> 1, 2 (licit) -> 0, unknown -> NaN\n",
    "df['class'] = df['class'].map({'1': 1, '2': 0})\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "# Features: Time step + f1 to f165\n",
    "X_cols = ['Time step'] + [f'f{i}' for i in range(1, 166)]\n",
    "X_all = df[X_cols].values\n",
    "y_all = df['class'].values # Contains NaN for unknowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['txId', 'Time step', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8',\n",
      "       ...\n",
      "       'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164',\n",
      "       'f165'],\n",
      "      dtype='object', length=167)\n",
      "(203769, 2)\n",
      "(203768, 167)\n"
     ]
    }
   ],
   "source": [
    "print(features_df.columns)\n",
    "print(classes_df.shape)\n",
    "print(features_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(203768,)\n",
      "(203768, 95)\n",
      "(203768, 167)\n"
     ]
    }
   ],
   "source": [
    "X = features_df.iloc[:, 0:95]\n",
    "y = classes_df[0:-1][\"class\"]\n",
    "print(y.shape)\n",
    "print(X.shape)\n",
    "print(features_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features (fit only on training data, but here we apply to all for unsupervised)\n",
    "print(\"Scaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X_all_scaled = scaler.fit_transform(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Assume previous sections have run successfully ---\n",
    "# 1. Data Loading (features_df, classes_df merged into df)\n",
    "# 2. Feature Scaling (X_all_scaled created)\n",
    "# 3. Label Preparation (known_indices, y_known created)\n",
    "# --- Required variables from previous steps: ---\n",
    "# X_all_scaled: Numpy array of all scaled features (N_samples, N_features)\n",
    "# known_indices: List or array of indices corresponding to labeled data\n",
    "# y_known: Numpy array of true labels (0 or 1) for the known data\n",
    "\n",
    "# --- Supervised Baseline using RandomForest ---\n",
    "\n",
    "print(\"\\n--- Starting Supervised Baseline Training (RandomForest) ---\")\n",
    "\n",
    "if 'X_all_scaled' not in locals() or 'known_indices' not in locals() or 'y_known' not in locals():\n",
    "    print(\"Error: Prerequisite data (X_all_scaled, known_indices, y_known) not found. Ensure data loading and prep ran.\")\n",
    "    # Handle error appropriately, maybe exit()\n",
    "    exit()\n",
    "elif len(known_indices) == 0:\n",
    "    print(\"Error: No known labels available for supervised training.\")\n",
    "    exit()\n",
    "else:\n",
    "    # Select the features and labels for the known data\n",
    "    X_known_scaled = X_all_scaled[known_indices]\n",
    "    # y_known is already defined\n",
    "\n",
    "    print(f\"Using {len(y_known)} labeled samples for training and testing.\")\n",
    "    print(f\"Feature shape: {X_known_scaled.shape}\")\n",
    "\n",
    "    # Split the labeled data into training and testing sets\n",
    "    # stratify=y_known ensures similar class proportions in train/test sets\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_known_scaled,\n",
    "            y_known,\n",
    "            test_size=0.3,       # Use 30% for testing\n",
    "            random_state=42,   # For reproducibility\n",
    "            stratify=y_known   # Important for imbalanced data\n",
    "        )\n",
    "        print(f\"Train set size: {len(y_train)}, Test set size: {len(y_test)}\")\n",
    "        print(f\"Illicit in train: {np.sum(y_train==1)}, Illicit in test: {np.sum(y_test==1)}\")\n",
    "\n",
    "    except ValueError as e:\n",
    "         print(f\"Error during train_test_split (potentially too few samples of one class for stratification): {e}\")\n",
    "         exit()\n",
    "\n",
    "\n",
    "    # Initialize RandomForestClassifier\n",
    "    # Hyperparameters can be tuned, these are reasonable defaults\n",
    "    # class_weight='balanced' helps with imbalanced data\n",
    "    rf_classifier = RandomForestClassifier(\n",
    "        n_estimators=150,      # Number of trees\n",
    "        max_depth=20,          # Limit tree depth to prevent overfitting (tune this)\n",
    "        random_state=42,\n",
    "        n_jobs=-1,             # Use all CPU cores\n",
    "        class_weight='balanced' # Important for imbalanced classes\n",
    "        # min_samples_leaf=5   # Can also help prevent overfitting\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print(\"\\nTraining RandomForest model...\")\n",
    "    start_time = time.time()\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    print(f\"RandomForest training completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    print(\"\\nMaking predictions on the test set...\")\n",
    "    y_pred_rf = rf_classifier.predict(X_test)\n",
    "    # Get probability predictions for AUROC calculation\n",
    "    # Predict probability of the positive class (illicit=1)\n",
    "    y_pred_proba_rf = rf_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Evaluate the results\n",
    "    print(\"\\n--- Supervised RandomForest Evaluation Results ---\")\n",
    "    accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "    precision_rf, recall_rf, f1_rf, _ = precision_recall_fscore_support(y_test, y_pred_rf, average='binary', pos_label=1, zero_division=0)\n",
    "\n",
    "    try:\n",
    "        # Check if both classes are present in y_test for AUROC\n",
    "        if len(np.unique(y_test)) > 1:\n",
    "             auroc_rf = roc_auc_score(y_test, y_pred_proba_rf) # Use probabilities for AUROC\n",
    "        else:\n",
    "             print(\"AUROC requires multiple classes in y_test. Skipping AUROC calculation.\")\n",
    "             auroc_rf = float('nan')\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not calculate AUROC: {e}\")\n",
    "        auroc_rf = float('nan')\n",
    "\n",
    "    print(f\"Accuracy (RF): {accuracy_rf:.4f}\")\n",
    "    print(f\"AUROC (RF):    {auroc_rf:.4f}\") # <<<--- THIS IS THE KEY BASELINE METRIC\n",
    "    print(f\"Precision (for illicit=1): {precision_rf:.4f}\")\n",
    "    print(f\"Recall (for illicit=1):    {recall_rf:.4f}\")\n",
    "    print(f\"F1-Score (for illicit=1):  {f1_rf:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report (RF - Test Set):\")\n",
    "    print(classification_report(y_test, y_pred_rf, target_names=[\"Licit (0)\", \"Illicit (1)\"], zero_division=0))\n",
    "\n",
    "    # Optional: Feature Importance\n",
    "    # importances = rf_classifier.feature_importances_\n",
    "    # feature_names = X_cols # From data loading section\n",
    "    # importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "    # importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "    # print(\"\\nTop 10 Feature Importances:\")\n",
    "    # print(importance_df.head(10))\n",
    "\n",
    "# --- Rest of your script (if any) ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
