{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2855529-6880-4473-9a9f-c79f00a00333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.2)\n",
      "Collecting kagglehub\n",
      "  Using cached kagglehub-0.3.12-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.5.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.4.1)\n",
      "Collecting lightly\n",
      "  Using cached lightly-1.5.20-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from kagglehub) (24.1)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from kagglehub) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from kagglehub) (4.66.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.11/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.68)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.11/site-packages (from lightly) (2024.8.30)\n",
      "Collecting hydra-core>=1.0.0 (from lightly)\n",
      "  Using cached hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting lightly_utils~=0.0.0 (from lightly)\n",
      "  Using cached lightly_utils-0.0.2-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.11/site-packages (from lightly) (1.16.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.11/site-packages (from lightly) (0.19.1)\n",
      "Requirement already satisfied: pydantic>=1.10.5 in /opt/conda/lib/python3.11/site-packages (from lightly) (2.8.2)\n",
      "Collecting pytorch_lightning>=1.0.4 (from lightly)\n",
      "  Using cached pytorch_lightning-2.5.1.post0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: urllib3>=1.25.3 in /opt/conda/lib/python3.11/site-packages (from lightly) (2.2.2)\n",
      "Collecting aenum>=3.1.11 (from lightly)\n",
      "  Using cached aenum-3.1.16-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting omegaconf<2.4,>=2.2 (from hydra-core>=1.0.0->lightly)\n",
      "  Using cached omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.0.0->lightly)\n",
      "  Using cached antlr4_python3_runtime-4.9.3-py3-none-any.whl\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic>=1.10.5->lightly) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.11/site-packages (from pydantic>=1.10.5->lightly) (2.20.1)\n",
      "Collecting torchmetrics>=0.7.0 (from pytorch_lightning>=1.0.4->lightly)\n",
      "  Using cached torchmetrics-1.7.2-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting lightning-utilities>=0.10.0 (from pytorch_lightning>=1.0.4->lightly)\n",
      "  Using cached lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->kagglehub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->kagglehub) (3.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly)\n",
      "  Downloading aiohttp-3.12.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from lightning-utilities>=0.10.0->pytorch_lightning>=1.0.4->lightly) (70.2.0)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly)\n",
      "  Using cached frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly)\n",
      "  Downloading multidict-6.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly)\n",
      "  Using cached propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly)\n",
      "  Using cached yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Using cached kagglehub-0.3.12-py3-none-any.whl (67 kB)\n",
      "Using cached lightly-1.5.20-py3-none-any.whl (851 kB)\n",
      "Using cached aenum-3.1.16-py3-none-any.whl (165 kB)\n",
      "Using cached hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "Using cached lightly_utils-0.0.2-py3-none-any.whl (6.4 kB)\n",
      "Using cached pytorch_lightning-2.5.1.post0-py3-none-any.whl (823 kB)\n",
      "Using cached lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
      "Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Downloading torchmetrics-1.7.2-py3-none-any.whl (962 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.5/962.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.12.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n",
      "Downloading multidict-6.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.7/223.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n",
      "Using cached yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
      "Installing collected packages: antlr4-python3-runtime, aenum, propcache, omegaconf, multidict, lightning-utilities, lightly_utils, frozenlist, aiohappyeyeballs, yarl, kagglehub, hydra-core, aiosignal, aiohttp, torchmetrics, pytorch_lightning, lightly\n",
      "Successfully installed aenum-3.1.16 aiohappyeyeballs-2.6.1 aiohttp-3.12.6 aiosignal-1.3.2 antlr4-python3-runtime-4.9.3 frozenlist-1.6.0 hydra-core-1.3.2 kagglehub-0.3.12 lightly-1.5.20 lightly_utils-0.0.2 lightning-utilities-0.14.3 multidict-6.4.4 omegaconf-2.3.0 propcache-0.3.1 pytorch_lightning-2.5.1.post0 torchmetrics-1.7.2 yarl-1.20.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas kagglehub seaborn matplotlib scikit-learn torch lightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f5d6d72-3ab5-45ee-a1e0-c547870aad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, precision_recall_fscore_support\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from lightly.loss import NTXentLoss\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import gc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efd9bb34-65c8-45dd-b32b-096a656eb847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c85cc3c7-a543-48c7-b53b-9644c0e24cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURES = 35 #166  # Time step + 165 features\n",
    "EMBEDDING_DIM = 64\n",
    "ENCODER_EMBEDDING_DIM = 64\n",
    "PROJECTION_DIM = 16\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-3 \n",
    "TEMPERATURE = 0.1\n",
    "\n",
    "AUG_NOISE_LEVEL = 0.03\n",
    "AUG_MASK_FRACTION = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeabc6c9-a413-4777-a429-cb358954bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"LI-Small_Trans.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e13a1af-2bee-4789-89de-ca8e7967a751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                 Timestamp  From Bank    Account  To Bank  Account.1  \\\n",
      "0        2022/09/01 00:08         11  8000ECA90       11  8000ECA90   \n",
      "1        2022/09/01 00:21       3402  80021DAD0     3402  80021DAD0   \n",
      "2        2022/09/01 00:00         11  8000ECA90     1120  8006AA910   \n",
      "3        2022/09/01 00:16       3814  8006AD080     3814  8006AD080   \n",
      "4        2022/09/01 00:00         20  8006AD530       20  8006AD530   \n",
      "...                   ...        ...        ...      ...        ...   \n",
      "6924044  2022/09/10 23:39      71696  81B2518F1    71528  81C0482E1   \n",
      "6924045  2022/09/10 23:48     271241  81B567481   173457  81C0DA751   \n",
      "6924046  2022/09/10 23:50     271241  81B567481   173457  81C0DA751   \n",
      "6924047  2022/09/10 23:57     170558  81A2206B1   275798  81C1D5CA1   \n",
      "6924048  2022/09/10 23:31     170558  81A2206B1   275798  81C1D5CA1   \n",
      "\n",
      "         Amount Received Receiving Currency   Amount Paid Payment Currency  \\\n",
      "0           3.195403e+06          US Dollar  3.195403e+06        US Dollar   \n",
      "1           1.858960e+03          US Dollar  1.858960e+03        US Dollar   \n",
      "2           5.925710e+05          US Dollar  5.925710e+05        US Dollar   \n",
      "3           1.232000e+01          US Dollar  1.232000e+01        US Dollar   \n",
      "4           2.941560e+03          US Dollar  2.941560e+03        US Dollar   \n",
      "...                  ...                ...           ...              ...   \n",
      "6924044     3.346900e-02            Bitcoin  3.346900e-02          Bitcoin   \n",
      "6924045     1.313000e-03            Bitcoin  1.313000e-03          Bitcoin   \n",
      "6924046     1.305800e-02            Bitcoin  1.305800e-02          Bitcoin   \n",
      "6924047     4.145370e-01            Bitcoin  4.145370e-01          Bitcoin   \n",
      "6924048     3.427700e-02            Bitcoin  3.427700e-02          Bitcoin   \n",
      "\n",
      "        Payment Format  Is Laundering  \n",
      "0         Reinvestment              0  \n",
      "1         Reinvestment              0  \n",
      "2               Cheque              0  \n",
      "3         Reinvestment              0  \n",
      "4         Reinvestment              0  \n",
      "...                ...            ...  \n",
      "6924044        Bitcoin              0  \n",
      "6924045        Bitcoin              0  \n",
      "6924046        Bitcoin              0  \n",
      "6924047        Bitcoin              0  \n",
      "6924048        Bitcoin              0  \n",
      "\n",
      "[6924049 rows x 11 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(dataset.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "916d51c4-bcba-47fd-bc1b-fc969819bf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocess_for_ssl(df_input):\n",
    "    \"\"\"\n",
    "    Prepares the IBM AML transaction data for self-supervised learning.\n",
    "    Converts all features to numerical format and scales them.\n",
    "    The 'Is Laundering' column is dropped from the features.\n",
    "    \"\"\"\n",
    "    print(\"Starting preprocessing...\")\n",
    "    df = df_input.copy()\n",
    "\n",
    "    if 'Is Laundering' in df.columns:\n",
    "        print(\"Dropping 'Is Laundering' column for SSL feature preparation.\")\n",
    "        df = df.drop('Is Laundering', axis=1)\n",
    "\n",
    "    # 1. Timestamp Processing\n",
    "    print(\"Processing Timestamp...\")\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce') # Coerce errors\n",
    "    df.dropna(subset=['Timestamp'], inplace=True) # Drop rows where timestamp couldn't be parsed\n",
    "\n",
    "    df['Time_Hour'] = df['Timestamp'].dt.hour\n",
    "    df['Time_Minute'] = df['Timestamp'].dt.minute\n",
    "    df['Time_DayOfWeek'] = df['Timestamp'].dt.dayofweek\n",
    "    df['Time_DayOfMonth'] = df['Timestamp'].dt.day\n",
    "    df['Time_Month'] = df['Timestamp'].dt.month\n",
    "\n",
    "    df['Hour_sin'] = np.sin(2 * np.pi * df['Time_Hour'] / 24.0)\n",
    "    df['Hour_cos'] = np.cos(2 * np.pi * df['Time_Hour'] / 24.0)\n",
    "    df['Minute_sin'] = np.sin(2 * np.pi * df['Time_Minute'] / 60.0)\n",
    "    df['Minute_cos'] = np.cos(2 * np.pi * df['Time_Minute'] / 60.0)\n",
    "    df['DayOfWeek_sin'] = np.sin(2 * np.pi * df['Time_DayOfWeek'] / 7.0)\n",
    "    df['DayOfWeek_cos'] = np.cos(2 * np.pi * df['Time_DayOfWeek'] / 7.0)\n",
    "    df['Month_sin'] = np.sin(2 * np.pi * df['Time_Month'] / 12.0)\n",
    "    df['Month_cos'] = np.cos(2 * np.pi * df['Time_Month'] / 12.0)\n",
    "    df = df.drop(['Timestamp', 'Time_Hour', 'Time_Minute', 'Time_DayOfWeek', 'Time_DayOfMonth', 'Time_Month'], axis=1)\n",
    "\n",
    "    # 2. Account Number Processing\n",
    "    print(\"Processing Account Numbers...\")\n",
    "    def hex_to_int_safe(hex_str):\n",
    "        try: return int(str(hex_str), 16)\n",
    "        except: return -1 # Use a numeric placeholder\n",
    "    df['Account_Num'] = df['Account'].apply(hex_to_int_safe)\n",
    "    df['Account.1_Num'] = df['Account.1'].apply(hex_to_int_safe)\n",
    "    df = df.drop(['Account', 'Account.1'], axis=1)\n",
    "\n",
    "    # 3. Amount Processing\n",
    "    print(\"Processing Amounts...\")\n",
    "    amount_col_to_use = 'Amount Received'\n",
    "    currency_col_to_use = 'Receiving Currency'\n",
    "\n",
    "    # Ensure amount columns are numeric, coercing errors\n",
    "    df['Amount Received'] = pd.to_numeric(df['Amount Received'], errors='coerce')\n",
    "    df['Amount Paid'] = pd.to_numeric(df['Amount Paid'], errors='coerce')\n",
    "    df.dropna(subset=['Amount Received', 'Amount Paid'], inplace=True) # Drop rows with non-numeric amounts\n",
    "\n",
    "    if (df['Amount Received'] == df['Amount Paid']).all() and \\\n",
    "       (df['Receiving Currency'] == df['Payment Currency']).all():\n",
    "        df = df.drop(['Amount Paid', 'Payment Currency'], axis=1)\n",
    "    else:\n",
    "        print(\"Warning: Amount/Currency pairs are not always identical. Dropping 'Amount Paid' & 'Payment Currency'.\")\n",
    "        if 'Amount Paid' in df.columns: df = df.drop('Amount Paid', axis=1)\n",
    "        if 'Payment Currency' in df.columns: df = df.drop('Payment Currency', axis=1)\n",
    "\n",
    "    df['Amount_Log'] = np.log1p(df[amount_col_to_use])\n",
    "    df = df.drop([amount_col_to_use], axis=1)\n",
    "\n",
    "    # 4. Categorical String Features: Currency and Payment Format\n",
    "    print(\"Processing Categorical String Features (Currency, Payment Format)...\")\n",
    "    categorical_to_encode = [currency_col_to_use, 'Payment Format']\n",
    "    # Ensure these columns are strings before get_dummies\n",
    "    for col in categorical_to_encode:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str) # Convert to string to handle mixed types or NaNs gracefully\n",
    "\n",
    "    df = pd.get_dummies(df, columns=categorical_to_encode, prefix=['Currency', 'Format'], dummy_na=False, dtype=float) # Use float for dummies\n",
    "\n",
    "    # --- Intermediate Check: Ensure all columns are numeric before scaling ---\n",
    "    print(\"\\nData types before final scaling:\")\n",
    "    print(df.dtypes)\n",
    "\n",
    "    # Identify any remaining object columns\n",
    "    object_cols = df.select_dtypes(include='object').columns\n",
    "    if len(object_cols) > 0:\n",
    "        print(f\"Warning: Found object columns after get_dummies: {object_cols.tolist()}\")\n",
    "        print(\"Attempting to convert them to numeric, or dropping them if conversion fails.\")\n",
    "        for col in object_cols:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce') # Try to convert\n",
    "        df.dropna(inplace=True) # Drop rows where conversion created NaNs if critical\n",
    "\n",
    "    # Re-check after attempted conversion\n",
    "    final_object_cols = df.select_dtypes(include='object').columns\n",
    "    if len(final_object_cols) > 0:\n",
    "        print(f\"ERROR: Still have object columns: {final_object_cols.tolist()}. These will cause issues.\")\n",
    "        print(\"Problematic columns' unique values (first few):\")\n",
    "        for col in final_object_cols:\n",
    "            print(f\"Column '{col}': {df[col].unique()[:5]}\")\n",
    "        # Decide how to handle: drop them, or fix their conversion\n",
    "        print(f\"Dropping problematic object columns: {final_object_cols.tolist()}\")\n",
    "        df = df.drop(columns=final_object_cols)\n",
    "\n",
    "\n",
    "    # 5. Scaling all numerical features\n",
    "    print(\"\\nScaling all numerical features...\")\n",
    "    numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "    if not numerical_cols:\n",
    "        print(\"Error: No numerical columns found to scale. Check preprocessing steps.\")\n",
    "        return np.array([]) # Or raise an error\n",
    "\n",
    "    print(f\"Columns to be scaled: {numerical_cols}\")\n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "    print(f\"\\nPreprocessing finished. Final feature shape: {df.shape}\")\n",
    "    print(\"Final data types after all processing:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"Sample of processed data (head):\")\n",
    "    print(df.head())\n",
    "\n",
    "    return df.values.astype(np.float32) # Explicitly cast to float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3d2f163-30ff-4280-8f2c-9109e656b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourAugmentedDataset(Dataset):\n",
    "    def __init__(self, features_np_array, noise_level=0.05, mask_fraction=0.15):\n",
    "        if features_np_array.dtype != np.float32: features_np_array = features_np_array.astype(np.float32)\n",
    "        self.features = torch.tensor(features_np_array, dtype=torch.float32)\n",
    "        self.noise_level = noise_level; self.mask_fraction = mask_fraction\n",
    "        self.n_features_dim = features_np_array.shape[1]\n",
    "    def __len__(self): return len(self.features)\n",
    "    def _augment(self, x_original):\n",
    "        x = x_original.clone()\n",
    "        if self.noise_level > 0: x += torch.randn_like(x) * self.noise_level\n",
    "        if self.mask_fraction > 0: x *= (torch.rand(self.n_features_dim, device=x.device) > self.mask_fraction).float()\n",
    "        return x\n",
    "    def __getitem__(self, idx):\n",
    "        original_x = self.features[idx]; view1 = self._augment(original_x); view2 = self._augment(original_x)\n",
    "        return view1, view2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a691f3e8-a18b-419b-b478-2aaaff40bb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, features_np_array):\n",
    "        if features_np_array.dtype != np.float32: features_np_array = features_np_array.astype(np.float32)\n",
    "        self.features = torch.tensor(features_np_array, dtype=torch.float32)\n",
    "    def __len__(self): return len(self.features)\n",
    "    def __getitem__(self, idx): return self.features[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7a2ce4-b0ed-42dc-a2ca-b224b20973f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing...\n",
      "Dropping 'Is Laundering' column for SSL feature preparation.\n",
      "Processing Timestamp...\n",
      "Processing Account Numbers...\n",
      "Processing Amounts...\n",
      "Warning: Amount/Currency pairs are not always identical. Dropping 'Amount Paid' & 'Payment Currency'.\n",
      "Processing Categorical String Features (Currency, Payment Format)...\n",
      "\n",
      "Data types before final scaling:\n",
      "From Bank                       int64\n",
      "To Bank                         int64\n",
      "Hour_sin                      float64\n",
      "Hour_cos                      float64\n",
      "Minute_sin                    float64\n",
      "Minute_cos                    float64\n",
      "DayOfWeek_sin                 float64\n",
      "DayOfWeek_cos                 float64\n",
      "Month_sin                     float64\n",
      "Month_cos                     float64\n",
      "Account_Num                     int64\n",
      "Account.1_Num                   int64\n",
      "Amount_Log                    float64\n",
      "Currency_Australian Dollar    float64\n",
      "Currency_Bitcoin              float64\n",
      "Currency_Brazil Real          float64\n",
      "Currency_Canadian Dollar      float64\n",
      "Currency_Euro                 float64\n",
      "Currency_Mexican Peso         float64\n",
      "Currency_Ruble                float64\n",
      "Currency_Rupee                float64\n",
      "Currency_Saudi Riyal          float64\n",
      "Currency_Shekel               float64\n",
      "Currency_Swiss Franc          float64\n",
      "Currency_UK Pound             float64\n",
      "Currency_US Dollar            float64\n",
      "Currency_Yen                  float64\n",
      "Currency_Yuan                 float64\n",
      "Format_ACH                    float64\n",
      "Format_Bitcoin                float64\n",
      "Format_Cash                   float64\n",
      "Format_Cheque                 float64\n",
      "Format_Credit Card            float64\n",
      "Format_Reinvestment           float64\n",
      "Format_Wire                   float64\n",
      "dtype: object\n",
      "\n",
      "Scaling all numerical features...\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv(\"LI-Small_Trans.csv\")\n",
    "\n",
    "X_processed_features_np = preprocess_for_ssl(df_raw)\n",
    "\n",
    "# Check the dtype of the resulting NumPy array\n",
    "if X_processed_features_np.size > 0: \n",
    "    print(f\"\\nDtype of X_processed_features_np: {X_processed_features_np.dtype}\")\n",
    "    if X_processed_features_np.dtype == np.object_:\n",
    "        print(\"ERROR: X_processed_features_np still has object dtype after preprocessing!\")\n",
    "    else:\n",
    "        print(\"SUCCESS: X_processed_features_np is now a numerical type.\")\n",
    "else:\n",
    "    print(\"Warning: Preprocessing resulted in an empty array. Check for excessive NaN dropping.\")\n",
    "\n",
    "# Now try creating your dataset again\n",
    "if X_processed_features_np.size > 0 and X_processed_features_np.dtype != np.object_:\n",
    "    # Make sure YourAugmentedDataset and torch are defined\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset # Assuming YourAugmentedDataset inherits from this\n",
    "\n",
    "    # (Paste your YourAugmentedDataset class definition here if not already defined)\n",
    "    class YourAugmentedDataset(Dataset):\n",
    "        def __init__(self, features_np_array, noise_level=0.05, mask_fraction=0.15):\n",
    "            # Ensure features_np_array is float32 before converting to tensor\n",
    "            if features_np_array.dtype != np.float32:\n",
    "                print(f\"Warning: features_np_array dtype is {features_np_array.dtype}, converting to np.float32.\")\n",
    "                features_np_array = features_np_array.astype(np.float32)\n",
    "            \n",
    "            self.features = torch.tensor(features_np_array, dtype=torch.float32)\n",
    "            self.noise_level = noise_level\n",
    "            self.mask_fraction = mask_fraction\n",
    "            self.n_samples, self.n_features_dim = features_np_array.shape\n",
    "            print(f\"Augmented Dataset: {self.n_samples} samples, {self.n_features_dim} features.\")\n",
    "            print(f\"Augmentation Config: Noise Level={self.noise_level}, Mask Fraction={self.mask_fraction}\")\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.features)\n",
    "\n",
    "        def _augment(self, x_original):\n",
    "            x = x_original.clone()\n",
    "            if self.noise_level > 0:\n",
    "                noise = torch.randn_like(x) * self.noise_level\n",
    "                x = x + noise\n",
    "            if self.mask_fraction > 0:\n",
    "                mask = (torch.rand(self.n_features_dim, device=x.device) > self.mask_fraction).float()\n",
    "                x = x * mask\n",
    "            return x\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            original_x = self.features[idx]\n",
    "            view1 = self._augment(original_x)\n",
    "            view2 = self._augment(original_x)\n",
    "            return view1, view2\n",
    "\n",
    "    augmented_dataset = YourAugmentedDataset(\n",
    "        features_np_array=X_processed_features_np,\n",
    "        noise_level=0.05,\n",
    "        mask_fraction=0.15\n",
    "    )\n",
    "    print(\"Dataset created successfully.\")\n",
    "else:\n",
    "    print(\"Skipping dataset creation due to preprocessing issues or empty array.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5f04c3-2bfe-45b3-aa90-4cb1852470b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import math # Keep for now, though not strictly needed for length calculation without pooling\n",
    "\n",
    "# class CNNGRUEncoder(nn.Module):\n",
    "#     def __init__(self, input_dim, cnn_channels1=32, cnn_channels2=64, cnn_channels3=128, cnn_channels4=256,\n",
    "#                  kernel_size=9, # pool_kernel is no longer used\n",
    "#                  gru_hidden_size=128, gru_layers=4, bidirectional=True,\n",
    "#                  embedding_dim=64):\n",
    "#         super(CNNGRUEncoder, self).__init__()\n",
    "#         self.input_dim = input_dim # This is N_FEATURES_PROC\n",
    "\n",
    "#         # CNN Layers (No Pooling)\n",
    "#         self.conv1 = nn.Conv1d(1, cnn_channels1, kernel_size, padding='same')\n",
    "#         self.relu1 = nn.ReLU()\n",
    "#         # self.pool1 = nn.MaxPool1d(pool_kernel) # Commented out\n",
    "\n",
    "#         self.conv2 = nn.Conv1d(cnn_channels1, cnn_channels2, kernel_size, padding='same')\n",
    "#         self.relu2 = nn.ReLU()\n",
    "#         # self.pool2 = nn.MaxPool1d(pool_kernel) # Commented out\n",
    "\n",
    "#         self.conv3 = nn.Conv1d(cnn_channels2, cnn_channels3, kernel_size, padding='same')\n",
    "#         self.relu3 = nn.ReLU()\n",
    "#         # self.pool3 = nn.MaxPool1d(pool_kernel) # Commented out\n",
    "\n",
    "#         self.conv4 = nn.Conv1d(cnn_channels3, cnn_channels4, kernel_size, padding='same')\n",
    "#         self.relu4 = nn.ReLU()\n",
    "#         # self.pool4 = nn.MaxPool1d(pool_kernel) # Assuming you might have had a 4th pool, also commented\n",
    "\n",
    "#         # Since padding='same' and stride=1 (default for Conv1d) and no pooling,\n",
    "#         # the sequence length remains self.input_dim throughout the CNNs.\n",
    "#         self.cnn_output_length = self.input_dim\n",
    "#         self.cnn_output_channels = cnn_channels4 # Channels from the last conv layer\n",
    "\n",
    "#         print(f\"CNN Configuration: Input Seq Length = {self.input_dim}\")\n",
    "#         print(f\"CNN Output (to GRU): Sequence Length = {self.cnn_output_length}, Features per step (Channels) = {self.cnn_output_channels}\")\n",
    "\n",
    "#         # GRU Layer\n",
    "#         # Input to GRU: (batch_size, sequence_length=self.cnn_output_length, features_per_step=self.cnn_output_channels)\n",
    "#         self.gru = nn.GRU(\n",
    "#             input_size=self.cnn_output_channels, # Features per time step from CNN\n",
    "#             hidden_size=gru_hidden_size,\n",
    "#             num_layers=gru_layers,\n",
    "#             batch_first=True,\n",
    "#             bidirectional=bidirectional\n",
    "#         )\n",
    "        \n",
    "#         # Output FC Layer\n",
    "#         gru_output_dim_for_fc = gru_hidden_size * (2 if bidirectional else 1)\n",
    "#         self.fc_out = nn.Linear(gru_output_dim_for_fc, embedding_dim)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Input x: (batch_size, N_FEATURES_PROC)\n",
    "#         # Reshape for Conv1d: (batch_size, in_channels=1, sequence_length=N_FEATURES_PROC)\n",
    "#         x = x.unsqueeze(1) # x is now (batch_size, 1, self.input_dim)\n",
    "\n",
    "#         # Pass through CNN layers\n",
    "#         x = self.relu1(self.conv1(x))\n",
    "#         x = self.relu2(self.conv2(x))\n",
    "#         x = self.relu3(self.conv3(x))\n",
    "#         x = self.relu4(self.conv4(x))\n",
    "#         # x is now (batch_size, self.cnn_output_channels, self.input_dim)\n",
    "\n",
    "#         # Prepare for GRU: (batch_size, sequence_length, features_per_step)\n",
    "#         # Current x: (batch_size, channels, sequence_length_after_cnn)\n",
    "#         # Need to permute to: (batch_size, sequence_length_after_cnn, channels)\n",
    "#         x = x.permute(0, 2, 1) # x is now (batch_size, self.input_dim, self.cnn_output_channels)\n",
    "\n",
    "#         # Pass through GRU\n",
    "#         # self.gru.flatten_parameters() # Good practice if using DataParallel/DDP, or if you see warnings\n",
    "#         _, h_n = self.gru(x) # out_gru shape: (batch, seq_len, num_directions * hidden_size)\n",
    "#                              # h_n shape: (num_layers * num_directions, batch, hidden_size)\n",
    "        \n",
    "#         # Extract the last hidden state (or combined last hidden states for bidirectional)\n",
    "#         if self.gru.bidirectional:\n",
    "#             # h_n is (num_layers*2, batch, hidden_size)\n",
    "#             # Last forward is h_n[-2,:,:] and last backward is h_n[-1,:,:]\n",
    "#             # These are from the *last layer* of the GRU stack.\n",
    "#             gru_out_for_fc = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n",
    "#         else:\n",
    "#             # h_n is (num_layers, batch, hidden_size)\n",
    "#             # Last hidden state from the last layer.\n",
    "#             gru_out_for_fc = h_n[-1,:,:]\n",
    "        \n",
    "#         # Pass through output FC layer\n",
    "#         embedding = self.fc_out(gru_out_for_fc) # gru_out_for_fc is (batch_size, fc_in_features)\n",
    "        \n",
    "#         # Normalize the final embedding (L2 normalization)\n",
    "#         embedding = F.normalize(embedding, p=2, dim=1)\n",
    "        \n",
    "#         return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e1d5b0-072e-4359-902c-c4c49d9c32fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransactionFeatureCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN to process features of a SINGLE transaction and produce an embedding for it.\n",
    "    This is the \"inner\" CNN.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_input_features_per_transaction, # N_FEATURES_PROC\n",
    "                 cnn_channels=[32, 64, 128],       # Channels for CNN layers\n",
    "                 kernel_sizes=[5, 5, 3],           # Kernel sizes\n",
    "                 output_embedding_dim=128):        # Embedding size for one transaction\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_input_features = num_input_features_per_transaction\n",
    "        cnn_layers = []\n",
    "        in_channels = 1 # We process features of one transaction as a 1-channel sequence\n",
    "\n",
    "        current_seq_len = num_input_features_per_transaction\n",
    "        for i, out_c in enumerate(cnn_channels):\n",
    "            cnn_layers.append(nn.Conv1d(in_channels, out_c, kernel_sizes[i], padding='same'))\n",
    "            cnn_layers.append(nn.ReLU())\n",
    "            # Optional: Pooling if num_input_features_per_transaction is large\n",
    "            # For example, if current_seq_len > kernel_sizes[i] * 2:\n",
    "            #     pool_k = 2\n",
    "            #     cnn_layers.append(nn.MaxPool1d(pool_k))\n",
    "            #     current_seq_len //= pool_k\n",
    "            in_channels = out_c\n",
    "            \n",
    "        self.cnn_block = nn.Sequential(*cnn_layers)\n",
    "        \n",
    "        # Calculate the flattened size after CNNs (if no pooling, it's cnn_channels[-1] * num_input_features)\n",
    "        # If pooling is added, this needs to be calculated based on pooling effects\n",
    "        # For simplicity, assume global average pooling to get fixed size\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(1) # Output (batch, cnn_channels[-1], 1)\n",
    "        self.fc_out_cnn = nn.Linear(cnn_channels[-1], output_embedding_dim)\n",
    "\n",
    "        print(f\"TransactionFeatureCNN: InputFeat={num_input_features_per_transaction}, OutputEmb={output_embedding_dim}\")\n",
    "\n",
    "    def forward(self, x_single_transaction_features):\n",
    "        # Input x_single_transaction_features: (batch_size_of_transactions, num_input_features_per_transaction)\n",
    "        \n",
    "        # Reshape for Conv1d: (batch_size_of_transactions, 1, num_input_features_per_transaction)\n",
    "        x = x_single_transaction_features.unsqueeze(1) \n",
    "        \n",
    "        x = self.cnn_block(x) # (batch_size_of_transactions, cnn_channels[-1], num_input_features_or_reduced)\n",
    "        x = self.adaptive_pool(x) # (batch_size_of_transactions, cnn_channels[-1], 1)\n",
    "        x = x.squeeze(-1) # (batch_size_of_transactions, cnn_channels[-1])\n",
    "        \n",
    "        transaction_embedding = self.fc_out_cnn(x)\n",
    "        return transaction_embedding\n",
    "\n",
    "\n",
    "class TransactionSequenceEncoder_CNNthenGRU(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_features_per_transaction, # N_FEATURES_PROC\n",
    "                 # Inner CNN params\n",
    "                 cnn_internal_channels=[32, 64],\n",
    "                 cnn_internal_kernel_sizes=[5, 3],\n",
    "                 transaction_embedding_dim=128, # Output of inner CNN, input to GRU\n",
    "                 # GRU params\n",
    "                 gru_hidden_size=256, \n",
    "                 gru_layers=2, \n",
    "                 gru_bidirectional=True,\n",
    "                 # Final embedding for the whole sequence\n",
    "                 final_sequence_embedding_dim=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transaction_cnn_embedder = TransactionFeatureCNN(\n",
    "            num_input_features_per_transaction=num_features_per_transaction,\n",
    "            cnn_channels=cnn_internal_channels,\n",
    "            kernel_sizes=cnn_internal_kernel_sizes,\n",
    "            output_embedding_dim=transaction_embedding_dim\n",
    "        )\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=transaction_embedding_dim, # Takes embeddings of individual transactions\n",
    "            hidden_size=gru_hidden_size,\n",
    "            num_layers=gru_layers,\n",
    "            batch_first=True, # Expects (batch, seq_len, features)\n",
    "            bidirectional=gru_bidirectional\n",
    "        )\n",
    "        \n",
    "        gru_output_dim_for_fc = gru_hidden_size * (2 if gru_bidirectional else 1)\n",
    "        self.fc_out_sequence = nn.Linear(gru_output_dim_for_fc, final_sequence_embedding_dim)\n",
    "\n",
    "        print(f\"TransactionSequenceEncoder_CNNthenGRU: FinalEmb={final_sequence_embedding_dim}\")\n",
    "        \n",
    "    def forward(self, x_batch_of_sequences):\n",
    "        # Input x_batch_of_sequences: (batch_size, sequence_length=10, num_features_per_transaction)\n",
    "        \n",
    "        batch_size, seq_len, num_features = x_batch_of_sequences.shape\n",
    "        \n",
    "        # To process each transaction in the sequence with the CNN:\n",
    "        # 1. Reshape to treat all transactions across all sequences in the batch as one big batch for the CNN\n",
    "        #    (batch_size * seq_len, num_features_per_transaction)\n",
    "        x_flat_transactions = x_batch_of_sequences.reshape(batch_size * seq_len, num_features)\n",
    "        \n",
    "        # 2. Get embeddings for all transactions\n",
    "        #    Output shape: (batch_size * seq_len, transaction_embedding_dim)\n",
    "        transaction_embeddings_flat = self.transaction_cnn_embedder(x_flat_transactions)\n",
    "        \n",
    "        # 3. Reshape back to sequence format for the GRU\n",
    "        #    Output shape: (batch_size, seq_len, transaction_embedding_dim)\n",
    "        transaction_embeddings_sequence = transaction_embeddings_flat.reshape(batch_size, seq_len, -1)\n",
    "        \n",
    "        # 4. Pass sequence of transaction embeddings through GRU\n",
    "        # self.gru.flatten_parameters()\n",
    "        _, h_n = self.gru(transaction_embeddings_sequence)\n",
    "        # h_n shape: (num_gru_layers * num_directions, batch_size, gru_hidden_size)\n",
    "        \n",
    "        if self.gru.bidirectional:\n",
    "            # Concatenate the last hidden states of the last GRU layer\n",
    "            gru_out_for_fc = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            gru_out_for_fc = h_n[-1,:,:]\n",
    "        # gru_out_for_fc shape: (batch_size, gru_hidden_size * num_directions)\n",
    "            \n",
    "        sequence_embedding = self.fc_out_sequence(gru_out_for_fc)\n",
    "        \n",
    "        # Optional: Normalize final sequence embedding\n",
    "        sequence_embedding = F.normalize(sequence_embedding, p=2, dim=1)\n",
    "        \n",
    "        return sequence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c465ba70-8e65-49b5-abb9-162510e197d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim=ENCODER_EMBEDDING_DIM, hidden_dim=ENCODER_EMBEDDING_DIM, output_dim=PROJECTION_DIM):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        # Simple 2-layer MLP as projection head\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        print(f\"Projection Head: Input={input_dim}, Hidden={hidden_dim}, Output={output_dim}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        # Note: NTXentLoss often includes normalization internally, or you can add it here\n",
    "        # x = F.normalize(x, p=2, dim=1) # Optional normalization here\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42549b0-02a4-41a4-82ee-1e6b9516aec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate encoder\n",
    "encoder = TransactionSequenceEncoder_CNNthenGRU(\n",
    "    num_features_per_transaction=N_FEATURES_PROC,\n",
    "    cnn_internal_channels=[32, 64],       # Example\n",
    "    cnn_internal_kernel_sizes=[5, 3],     # Example\n",
    "    transaction_embedding_dim=128,        # Example: output of CNN per transaction\n",
    "    gru_hidden_size=256,                  # Example\n",
    "    gru_layers=2,                         # Example\n",
    "    final_sequence_embedding_dim=ENCODER_EMBEDDING_DIM # Your desired final SSL embedding dim\n",
    ").to(DEVICE)\n",
    "# encoder = CNNGRUEncoder(input_dim=N_FEATURES, embedding_dim=EMBEDDING_DIM).to(DEVICE)\n",
    "encoder = torch.compile(encoder) # Compile the model after creating it\n",
    "optimizer = optim.Adam(encoder.parameters(), lr=LEARNING_RATE)\n",
    "print(\"\\nCNN-GRU Encoder Architecture:\")\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6410faa2-e51e-439e-a7ae-854fdaf71452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(z, temperature):\n",
    "    \"\"\"\n",
    "    Optimized implementation of NT-Xent loss using logsumexp.\n",
    "    Assumes z = torch.cat([view1, view2], dim=0)\n",
    "    where view1 and view2 have shape (B, D) and B is the batch size.\n",
    "    \"\"\"\n",
    "    n = z.shape[0] # Shape is (2*B, D)\n",
    "    if n < 2:\n",
    "        # Handle edge case where batch size is too small after drop_last\n",
    "        return torch.tensor(0.0, device=z.device, requires_grad=True)\n",
    "    batch_size = n // 2 # The original batch size B\n",
    "\n",
    "    # Calculate cosine similarity matrix (2B x 2B)\n",
    "    # Normalize features first is equivalent to cosine similarity for matrix mult\n",
    "    z_norm = F.normalize(z, p=2, dim=1)\n",
    "    sim_matrix = torch.mm(z_norm, z_norm.t()) # (2B, D) @ (D, 2B) -> (2B, 2B)\n",
    "    # Or using the function directly:\n",
    "    #sim_matrix = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2)\n",
    "\n",
    "\n",
    "    # Scale similarities by temperature\n",
    "    logits = sim_matrix / temperature\n",
    "\n",
    "    # --- Identify positive pairs ---\n",
    "    # Create labels identifying samples across the two views\n",
    "    labels = torch.arange(batch_size).to(z.device) # 0 to B-1\n",
    "    # Create a mask for positive pairs: (i, i+B) and (i+B, i)\n",
    "    mask_pos = torch.zeros_like(logits, dtype=torch.bool)\n",
    "    mask_pos[torch.arange(batch_size), torch.arange(batch_size) + batch_size] = True\n",
    "    mask_pos[torch.arange(batch_size) + batch_size, torch.arange(batch_size)] = True\n",
    "\n",
    "    # Extract the logits corresponding to positive pairs\n",
    "    # These are the sim(z1_k, z2_k)/T and sim(z2_k, z1_k)/T terms\n",
    "    positives = logits[mask_pos].view(n, 1) # Shape: (2B, 1)\n",
    "\n",
    "    # --- Calculate LogSumExp over negatives ---\n",
    "    # Mask out self-similarity (diagonal) for the logsumexp calculation\n",
    "    mask_self = torch.eye(n, dtype=torch.bool).to(z.device)\n",
    "    logits_masked = logits.masked_fill(mask_self, -float('inf')) # Exclude sim(i,i)\n",
    "\n",
    "    # Calculate logsumexp across all other samples (negatives + the other positive)\n",
    "    logsumexp_all = torch.logsumexp(logits_masked, dim=1, keepdim=True) # Shape: (2B, 1)\n",
    "\n",
    "    # --- Calculate final loss ---\n",
    "    # loss = log(sum(exp(negatives))) - positive_similarity\n",
    "    loss_per_sample = logsumexp_all - positives\n",
    "\n",
    "    # Average loss over all 2B samples (both views)\n",
    "    loss = loss_per_sample.mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780334c8-a0da-4e24-b0cc-5f4fdba61a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Creating augmented dataset with {X_processed_features_np.shape[0]} samples and {N_FEATURES} features...\")\n",
    "augmented_dataset = YourAugmentedDataset(\n",
    "    features_np_array=X_processed_features_np,\n",
    "    # Add your augmentation parameters here if they are not defaults\n",
    "    # noise_level=0.05,\n",
    "    # mask_fraction=0.15\n",
    ")\n",
    "\n",
    "# Create the DataLoader for training\n",
    "# num_workers > 0 can speed up data loading but might cause issues on some systems (e.g. Windows, Jupyter)\n",
    "# Start with num_workers=0 if you encounter problems.\n",
    "# pin_memory=True can speed up CPU to GPU data transfer if using CUDA.\n",
    "dataloader = DataLoader(\n",
    "    augmented_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2, # Adjust based on your system and if you see speedups/issues\n",
    "    pin_memory=(DEVICE.type == 'cuda'),\n",
    "    drop_last=True # Important for some contrastive losses if batch size consistency matters\n",
    "                  # Set to False if you want to process all samples even if the last batch is smaller\n",
    ")\n",
    "print(f\"Training DataLoader created: Batch Size={BATCH_SIZE}, Shuffle=True, Drop Last=True\")\n",
    "\n",
    "\n",
    "# Create the Dataset and DataLoader for evaluation/final embedding generation\n",
    "# This uses a SimpleDataset that does not apply augmentations.\n",
    "eval_dataset = SimpleDataset(X_processed_features_np)\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=BATCH_SIZE, # Often can use a larger batch size for inference\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=(DEVICE.type == 'cuda')\n",
    ")\n",
    "print(f\"Evaluation DataLoader created: Batch Size={BATCH_SIZE * 2}, Shuffle=False\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e495f3-6f59-4795-8514-5ee9b8a7ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Initialization, Optimizer, Loss ---\n",
    "print(f\"Initializing models on {DEVICE}...\")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == 'cuda'))\n",
    "\n",
    "# Instantiate BOTH models\n",
    "# Ensure N_FEATURES here is the number of features your preprocessed data has\n",
    "encoder = TransactionSequenceEncoder_CNNthenGRU(\n",
    "    num_features_per_transaction=N_FEATURES_PROC,\n",
    "    cnn_internal_channels=[32, 64],       # Example\n",
    "    cnn_internal_kernel_sizes=[5, 3],     # Example\n",
    "    transaction_embedding_dim=128,        # Example: output of CNN per transaction\n",
    "    gru_hidden_size=256,                  # Example\n",
    "    gru_layers=2,                         # Example\n",
    "    final_sequence_embedding_dim=ENCODER_EMBEDDING_DIM # Your desired final SSL embedding dim\n",
    ").to(DEVICE)\n",
    "\n",
    "# encoder = CNNGRUEncoder(input_dim=N_FEATURES, embedding_dim=ENCODER_EMBEDDING_DIM).to(DEVICE)\n",
    "projection_head = ProjectionHead(input_dim=ENCODER_EMBEDDING_DIM, output_dim=PROJECTION_DIM).to(DEVICE)\n",
    "\n",
    "# # Compile if desired (PyTorch 2.0+)\n",
    "if DEVICE.type == 'cuda' and hasattr(torch, 'compile'):\n",
    "    print(\"Attempting to compile models with torch.compile()...\")\n",
    "    try:\n",
    "        encoder = torch.compile(encoder)\n",
    "        projection_head = torch.compile(projection_head)\n",
    "        print(\"Models compiled successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"torch.compile failed: {e}. Proceeding without compilation.\")\n",
    "else:\n",
    "    print(\"torch.compile not used (either not CUDA or torch version < 2.0).\")\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    list(encoder.parameters()) + list(projection_head.parameters()), # Combine parameters\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=1e-6 # Example weight decay\n",
    ")\n",
    "\n",
    "# Use the library loss function (operates on projection head output)\n",
    "criterion = NTXentLoss(temperature=TEMPERATURE, memory_bank_size=0).to(DEVICE) # Ensure loss is on device if it has params\n",
    "\n",
    "print(\"\\n--- CNN-GRU Encoder Architecture ---\")\n",
    "print(encoder)\n",
    "print(\"\\n--- Projection Head Architecture ---\")\n",
    "print(projection_head)\n",
    "print(f\"\\nOptimizer: Adam, LR: {LEARNING_RATE}, Weight Decay: {1e-6}\")\n",
    "print(f\"Loss Criterion: NTXentLoss, Temperature: {TEMPERATURE}\")\n",
    "print(f\"Training for {EPOCHS} epochs. Batch size: {dataloader.batch_size if dataloader else 'N/A'}\")\n",
    "print(f\"Using Mixed Precision (AMP): {scaler.is_enabled()}\")\n",
    "\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(\"\\n--- Starting Unsupervised Training ---\")\n",
    "training_losses = [] # Renamed from 'losses' to avoid conflict if 'loss' is used later\n",
    "gradient_check_interval = 600 # Batches between grad checks, set to 0 or large to disable\n",
    "print_interval = 5000 # Batches between progress prints\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    encoder.train()\n",
    "    projection_head.train()\n",
    "\n",
    "    epoch_loss_sum = 0.0\n",
    "    batches_processed_in_epoch = 0\n",
    "    \n",
    "    epoch_data_time = 0.0\n",
    "    epoch_forward_time = 0.0\n",
    "    epoch_loss_calc_time = 0.0\n",
    "    epoch_backward_time = 0.0\n",
    "    epoch_optimizer_step_time = 0.0\n",
    "    epoch_grad_check_time = 0.0 # For the optional gradient check\n",
    "\n",
    "    epoch_overall_start_time = time.time()\n",
    "    batch_loop_start_time = time.time() # For data loading time of the first batch\n",
    "\n",
    "    for batch_idx, (view1, view2) in enumerate(dataloader):\n",
    "        batch_data_end_time = time.time()\n",
    "        epoch_data_time += (batch_data_end_time - batch_loop_start_time)\n",
    "\n",
    "        # Ensure batch is large enough for contrastive loss (NT-Xent often needs at least 2 distinct samples)\n",
    "        if view1.shape[0] < 2:\n",
    "            print(f\"Skipping batch {batch_idx+1} in epoch {epoch+1} due to insufficient size: {view1.shape[0]}\")\n",
    "            batch_loop_start_time = time.time() # Reset for next data load time\n",
    "            continue\n",
    "\n",
    "        view1, view2 = view1.to(DEVICE), view2.to(DEVICE)\n",
    "\n",
    "        # --- Forward Pass & Loss Calculation within Autocast ---\n",
    "        forward_pass_start_time = time.time()\n",
    "        with torch.amp.autocast(device_type=DEVICE.type, dtype=torch.float16, enabled=(DEVICE.type == 'cuda')):\n",
    "            z1_enc = encoder(view1)\n",
    "            z2_enc = encoder(view2)\n",
    "            p1 = projection_head(z1_enc)\n",
    "            p2 = projection_head(z2_enc)\n",
    "            \n",
    "            loss_calc_start_time = time.time()\n",
    "            current_batch_loss = criterion(p1, p2)\n",
    "            loss_calc_end_time = time.time()\n",
    "        forward_pass_end_time = loss_calc_start_time # Forward pass ends before loss calc starts\n",
    "        \n",
    "        epoch_forward_time += (forward_pass_end_time - forward_pass_start_time)\n",
    "        epoch_loss_calc_time += (loss_calc_end_time - loss_calc_start_time)\n",
    "\n",
    "        # --- Backpropagation & Optimizer Step ---\n",
    "        optimizer.zero_grad(set_to_none=True) # More memory efficient\n",
    "\n",
    "        if torch.isnan(current_batch_loss) or torch.isinf(current_batch_loss):\n",
    "            print(f\"WARNING: NaN/Inf loss detected at Epoch {epoch+1}, Batch {batch_idx+1}. Skipping update.\")\n",
    "            batch_loop_start_time = time.time() # Reset for next data load time\n",
    "            continue\n",
    "        \n",
    "        backward_pass_start_time = time.time()\n",
    "        scaler.scale(current_batch_loss).backward()\n",
    "        backward_pass_end_time = time.time()\n",
    "        epoch_backward_time += (backward_pass_end_time - backward_pass_start_time)\n",
    "\n",
    "        # Optional Gradient Checking\n",
    "        if gradient_check_interval > 0 and (batch_idx + 1) % gradient_check_interval == 0:\n",
    "            gc_start = time.time()\n",
    "            #print(f\"\\n--- Gradient Stats for Epoch {epoch+1}, Batch {batch_idx+1} ---\")\n",
    "            all_params = list(encoder.named_parameters()) + list(projection_head.named_parameters())\n",
    "            for name, param in all_params:\n",
    "                if param.grad is not None:\n",
    "                    grad_norm = param.grad.norm().item()\n",
    "                    grad_abs_mean = param.grad.abs().mean().item()\n",
    "                    # print(f\"{name:<50}: Grad Norm={grad_norm:.4e}, Grad Abs Mean={grad_abs_mean:.4e}\")\n",
    "                # else:\n",
    "                    # print(f\"{name:<50}: Grad is None\")\n",
    "            epoch_grad_check_time += (time.time() - gc_start)\n",
    "\n",
    "        optimizer_step_start_time = time.time()\n",
    "        # Optional: Gradient Clipping (Unscale first)\n",
    "        # if scaler.is_enabled(): scaler.unscale_(optimizer)\n",
    "        # torch.nn.utils.clip_grad_norm_(list(encoder.parameters()) + list(projection_head.parameters()), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer_step_end_time = time.time()\n",
    "        epoch_optimizer_step_time += (optimizer_step_end_time - optimizer_step_start_time)\n",
    "\n",
    "        epoch_loss_sum += current_batch_loss.item()\n",
    "        batches_processed_in_epoch += 1\n",
    "\n",
    "        if (batch_idx + 1) % print_interval == 0 or (batch_idx + 1) == len(dataloader):\n",
    "            if batches_processed_in_epoch > 0:\n",
    "                avg_loss_so_far = epoch_loss_sum / batches_processed_in_epoch\n",
    "                print(f\"Epoch [{epoch+1}/{EPOCHS}], Batch [{batch_idx+1}/{len(dataloader)}], Loss: {current_batch_loss.item():.4f}, Avg Epoch Loss: {avg_loss_so_far:.4f}\")\n",
    "        \n",
    "        batch_loop_start_time = time.time() # Reset for next data load time calculation\n",
    "\n",
    "    # --- End of Epoch Summary ---\n",
    "    epoch_overall_end_time = time.time()\n",
    "    epoch_duration = epoch_overall_end_time - epoch_overall_start_time\n",
    "\n",
    "    if batches_processed_in_epoch > 0:\n",
    "        avg_epoch_loss = epoch_loss_sum / batches_processed_in_epoch\n",
    "        training_losses.append(avg_epoch_loss)\n",
    "        print(f\"\\n--- Epoch [{epoch+1}/{EPOCHS}] Summary ---\")\n",
    "        print(f\"  Average Epoch Loss: {avg_epoch_loss:.4f}\")\n",
    "        print(f\"  Epoch Duration: {epoch_duration:.2f}s\")\n",
    "        print(f\"  Avg Time per Batch (Overall): {epoch_duration / batches_processed_in_epoch:.3f}s\")\n",
    "        # Print detailed timings (average per batch)\n",
    "        print(f\"  Avg Data Loading: {epoch_data_time / batches_processed_in_epoch:.4f}s\")\n",
    "        print(f\"  Avg Forward Pass: {epoch_forward_time / batches_processed_in_epoch:.4f}s\")\n",
    "        print(f\"  Avg Loss Calc:    {epoch_loss_calc_time / batches_processed_in_epoch:.4f}s\")\n",
    "        print(f\"  Avg Backward Pass: {epoch_backward_time / batches_processed_in_epoch:.4f}s\")\n",
    "        print(f\"  Avg Optimizer Step: {epoch_optimizer_step_time / batches_processed_in_epoch:.4f}s\")\n",
    "        if epoch_grad_check_time > 0:\n",
    "             num_grad_checks = batches_processed_in_epoch // gradient_check_interval if gradient_check_interval > 0 else 0\n",
    "             if num_grad_checks > 0:\n",
    "                 print(f\"  Avg Grad Check Time (per check): {epoch_grad_check_time / num_grad_checks:.4f}s\")\n",
    "    else:\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}] completed with no batches processed.\")\n",
    "        training_losses.append(float('nan')) # Or handle appropriately\n",
    "\n",
    "print(\"\\n--- Unsupervised Training Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c4e9ed-5273-4bc8-b8e6-226a8b011d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- At the beginning of embedding generation ---\n",
    "print(\"\\n--- Generating Final Embeddings (with pre-allocation attempt) ---\")\n",
    "encoder.eval()\n",
    "\n",
    "if 'eval_dataset' not in locals() or eval_dataset is None: # Make sure eval_dataset is defined\n",
    "    print(\"ERROR: eval_dataset is not defined!\")\n",
    "    # exit()\n",
    "else:\n",
    "    num_total_samples = len(eval_dataset)\n",
    "    # ENCODER_EMBEDDING_DIM needs to be correctly defined\n",
    "    # Example: ENCODER_EMBEDDING_DIM = encoder.fc_out.out_features (if fc_out is the last layer)\n",
    "    # Or pass it as a known constant\n",
    "    if 'ENCODER_EMBEDDING_DIM' not in locals():\n",
    "        print(\"ERROR: ENCODER_EMBEDDING_DIM not defined!\")\n",
    "        # exit()\n",
    "    else:\n",
    "        print(f\"DEBUG: Pre-allocating NumPy array for {num_total_samples} samples, {ENCODER_EMBEDDING_DIM} embedding dim.\")\n",
    "        # Ensure ENCODER_EMBEDDING_DIM is correct\n",
    "        final_embeddings_np = np.zeros((num_total_samples, ENCODER_EMBEDDING_DIM), dtype=np.float32)\n",
    "        current_idx = 0\n",
    "\n",
    "        if DEVICE.type == 'cuda': torch.cuda.empty_cache()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data_batch in enumerate(eval_dataloader): # Use your existing eval_dataloader\n",
    "                # ... (data loading and feature extraction from data_batch as before) ...\n",
    "                if isinstance(data_batch, (list, tuple)):\n",
    "                    batch_features = data_batch[0].to(DEVICE)\n",
    "                else:\n",
    "                    batch_features = data_batch.to(DEVICE)\n",
    "                \n",
    "                if (batch_idx + 1) % 1000 == 0: print(f\"  Processing batch {batch_idx + 1}/{len(eval_dataloader)}...\")\n",
    "\n",
    "                if batch_features.shape[0] == 0: continue\n",
    "\n",
    "                try:\n",
    "                    with torch.amp.autocast(device_type=DEVICE.type, dtype=torch.float16, enabled=(DEVICE.type == 'cuda')):\n",
    "                        current_embeddings_tensor = encoder(batch_features)\n",
    "                    \n",
    "                    batch_actual_size = current_embeddings_tensor.shape[0]\n",
    "                    # Place directly into the pre-allocated NumPy array\n",
    "                    final_embeddings_np[current_idx : current_idx + batch_actual_size] = current_embeddings_tensor.cpu().numpy()\n",
    "                    current_idx += batch_actual_size\n",
    "\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"  RUNTIME ERROR on batch {batch_idx + 1}: {e}\")\n",
    "                    if \"out of memory\" in str(e).lower(): print(\"    CUDA OOM error.\")\n",
    "                    break \n",
    "                except Exception as e_other:\n",
    "                    print(f\"  UNEXPECTED PYTHON ERROR on batch {batch_idx + 1}: {e_other}\")\n",
    "                    break\n",
    "        \n",
    "        if current_idx < num_total_samples:\n",
    "            print(f\"Warning: Only {current_idx} embeddings were filled out of {num_total_samples} expected.\")\n",
    "            final_embeddings_np = final_embeddings_np[:current_idx] # Trim if processing stopped early\n",
    "\n",
    "        print(f\"Generated final embeddings shape: {final_embeddings_np.shape}\")\n",
    "        # Now `final_embeddings_np` is your result, no need for all_embeddings_list or concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8456287-8995-455b-8760-eb1b85ef41f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stage 2: Prepare Labels for Evaluation ---\n",
    "from sklearn.metrics import average_precision_score\n",
    "y_known = None\n",
    "# This logic assumes X_processed_features_np exists and its row count matches what eval_dataset was based on.\n",
    "# And df_raw has the original labels.\n",
    "# **Crucially, y_known must align with the samples in final_embeddings_np**\n",
    "if 'X_processed_features_np' in locals() and 'df_raw' in locals() and 'Is Laundering' in df_raw.columns:\n",
    "    # If your preprocessing function `preprocess_for_ssl` returned y_target that corresponds\n",
    "    # to X_processed_features_np, it's better to use that.\n",
    "    # y_known = y_target_from_preprocessing \n",
    "    \n",
    "    # Fallback: using df_raw and assuming X_processed_features_np corresponds to the head of df_raw\n",
    "    # or that their lengths are made to match.\n",
    "    num_samples_for_labels = len(X_processed_features_np) if 'X_processed_features_np' in locals() else (len(final_embeddings_np) if final_embeddings_np is not None else 0)\n",
    "\n",
    "    if num_samples_for_labels > 0 and num_samples_for_labels <= len(df_raw):\n",
    "        y_known = df_raw['Is Laundering'].values[:num_samples_for_labels]\n",
    "        # known_indices = np.arange(len(y_known)) # Not strictly needed if y_known aligns directly\n",
    "        print(f\"Extracted {len(y_known)} labels for evaluation, matching {num_samples_for_labels} processed samples.\")\n",
    "    else:\n",
    "        print(f\"Warning: Cannot reliably extract y_known. num_samples_for_labels={num_samples_for_labels}, len(df_raw)={len(df_raw) if 'df_raw' in locals() else 'N/A'}\")\n",
    "elif 'y_target_from_preprocessing' in locals() and y_target_from_preprocessing is not None:\n",
    "    # Ideal case if your preprocessing function returns the aligned y_target\n",
    "    y_known = y_target_from_preprocessing\n",
    "    print(f\"Using y_target_from_preprocessing with {len(y_known)} labels.\")\n",
    "else:\n",
    "    print(\"Warning: Could not determine source for 'y_known'. Labels not available for evaluation.\")\n",
    "\n",
    "\n",
    "# --- Stage 3: Evaluate Embeddings with RandomForest Probe ---\n",
    "print(\"\\n--- Evaluating Learned Embeddings with RandomForest Probe ---\")\n",
    "\n",
    "# VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\n",
    "# Key Change: Use final_embeddings_np instead of all_embeddings\n",
    "# VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\n",
    "if final_embeddings_np is not None and y_known is not None and len(y_known) > 0:\n",
    "    if len(final_embeddings_np) != len(y_known):\n",
    "        print(f\"ERROR: Mismatch between number of embeddings ({len(final_embeddings_np)}) and labels ({len(y_known)}). Probe cannot proceed.\")\n",
    "        print(\"       Ensure y_known correctly corresponds to the samples for which embeddings were generated.\")\n",
    "    else:\n",
    "        embeddings_for_probe = final_embeddings_np # Use the correctly named variable\n",
    "        print(f\"Using {len(y_known)} labeled samples for RF probe.\")\n",
    "        print(f\"Embedding shape for probe: {embeddings_for_probe.shape}\")\n",
    "\n",
    "        try:\n",
    "            X_train_emb, X_test_emb, y_train_emb, y_test_emb = train_test_split(\n",
    "                embeddings_for_probe,\n",
    "                y_known,\n",
    "                test_size=0.3,\n",
    "                random_state=42,\n",
    "                stratify=y_known\n",
    "            )\n",
    "            print(f\"RF Probe: Train size={len(y_train_emb)}, Test size={len(y_test_emb)}\")\n",
    "            \n",
    "            # Check class distribution\n",
    "            if len(y_train_emb) > 0: print(f\"Class distribution in y_train_emb: {np.bincount(y_train_emb)}\")\n",
    "            if len(y_test_emb) > 0: print(f\"Class distribution in y_test_emb: {np.bincount(y_test_emb)}\")\n",
    "\n",
    "            if len(np.unique(y_train_emb)) < 2 or len(np.unique(y_test_emb)) < 2 :\n",
    "                print(\"Warning: One of the splits (train or test) for RF probe has only one class. Metrics might be misleading or fail.\")\n",
    "                # Proceeding, but be aware of potential issues with metrics.\n",
    "\n",
    "            rf_probe = RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=None,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                class_weight='balanced_subsample',\n",
    "                min_samples_leaf=5,\n",
    "                min_samples_split=10\n",
    "            )\n",
    "\n",
    "            print(\"\\nTraining RandomForest Probe...\")\n",
    "            fit_start_time = time.time()\n",
    "            rf_probe.fit(X_train_emb, y_train_emb)\n",
    "            fit_end_time = time.time()\n",
    "            print(f\"RF probe training completed in {fit_end_time - fit_start_time:.2f} seconds.\")\n",
    "\n",
    "            print(\"\\nEvaluating RF Probe on the test set...\")\n",
    "            y_pred_probe_rf = rf_probe.predict(X_test_emb)\n",
    "            y_pred_proba_probe_rf = rf_probe.predict_proba(X_test_emb)[:, 1]\n",
    "\n",
    "            accuracy_probe_rf = accuracy_score(y_test_emb, y_pred_probe_rf)\n",
    "            precision_probe_rf, recall_probe_rf, f1_probe_rf, _ = precision_recall_fscore_support(\n",
    "                y_test_emb, y_pred_probe_rf, average='binary', pos_label=1, zero_division=0\n",
    "            )\n",
    "                        \n",
    "            auprc_probe_rf = average_precision_score(y_test_emb, y_pred_proba_probe_rf, pos_label=1)\n",
    "            \n",
    "            auroc_probe_rf = float('nan')\n",
    "            if len(np.unique(y_test_emb)) > 1:\n",
    "                try: auroc_probe_rf = roc_auc_score(y_test_emb, y_pred_proba_probe_rf)\n",
    "                except ValueError as e_auroc: print(f\"Could not calculate AUROC for RF probe: {e_auroc}\")\n",
    "            else: print(\"AUROC cannot be calculated for RF probe: only one class in y_test_emb.\")\n",
    "\n",
    "            print(\"\\n--- RandomForest Probe Evaluation Results ---\")\n",
    "            print(f\"Accuracy (RF Probe): {accuracy_probe_rf:.4f}\")\n",
    "            print(f\"AUROC (RF Probe):    {auroc_probe_rf:.4f}\")\n",
    "            print(f\"AUPRC (Average Precision) (RF Probe): {auprc_probe_rf:.4f}\")\n",
    "            print(f\"Precision (Illicit): {precision_probe_rf:.4f}\")\n",
    "            print(f\"Recall (Illicit):    {recall_probe_rf:.4f}\")\n",
    "            print(f\"F1-Score (Illicit):  {f1_probe_rf:.4f}\")\n",
    "            print(\"\\nClassification Report (RF Probe - Test Set):\")\n",
    "            print(classification_report(y_test_emb, y_pred_probe_rf, target_names=[\"Licit (0)\", \"Illicit (1)\"], zero_division=0))\n",
    "\n",
    "            if hasattr(rf_probe, 'feature_importances_'):\n",
    "                importances_rf = rf_probe.feature_importances_\n",
    "                emb_indices = np.argsort(importances_rf)[::-1]\n",
    "                print(\"\\nTop 10 Embedding Dimension Importances (RF Probe):\")\n",
    "                for i in range(min(10, len(emb_indices))):\n",
    "                    print(f\"  Dim {emb_indices[i]}: {importances_rf[emb_indices[i]]:.4f}\")\n",
    "\n",
    "        except ValueError as e_split:\n",
    "            print(f\"Error during train/test split or RF evaluation: {e_split}\")\n",
    "        except Exception as e_general:\n",
    "            print(f\"An unexpected error occurred during RF probe evaluation: {e_general}\")\n",
    "else:\n",
    "    if final_embeddings_np is None:\n",
    "        print(\"\\nSkipping RF Probe evaluation: 'final_embeddings_np' was not generated successfully.\")\n",
    "    elif y_known is None or len(y_known) == 0:\n",
    "        print(\"\\nSkipping RF Probe evaluation: 'y_known' labels are not available or empty.\")\n",
    "\n",
    "gc.collect() # Clean up at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0598f6bc-0f2c-46ff-973b-aee1274c87c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
